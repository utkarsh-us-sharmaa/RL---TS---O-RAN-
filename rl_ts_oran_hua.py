# -*- coding: utf-8 -*-
"""RL TS ORAN-Hua.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fZ_oVHNCKODSCCp521G8TqUxAy_QhCxJ
"""

policy = {
    '1': {
        'p1': 'macro',
        'p2': 'pico',
        'p3': 'anchored'
    },
    '2': {
        'p1': 'macro',
        'p2': 'anchored',
        'p3': 'pico'
    },
    '3': {
        'p1': 'pico',
        'p2': 'anchored',
        'p3': 'macro'
    },
    '4': {
        'p1': 'anchored',
        'p2': 'macro',
        'p3': 'pico'
    }
}


import numpy as np
import random
class Environment:
    def __init__(self, ues, cells):
        self.ues = ues
        self.cells = cells
        self.reset()

    def reset(self):
        self.current_step = 0
        self.ues['allocatedCell'] = 0
        self.cells['currCapacity'] = self.cells['maxCapacity-Kbps']

    def step(self, action):
        ue_id, cell_id = action
        ue = self.ues.iloc[ue_id]
        cell = self.cells.iloc[cell_id]
        done = False

        if cell['currCapacity'] >= ue['throughput(Kbps)']:
            self.cells.loc[cell_id, 'currCapacity'] -= ue['throughput(Kbps)']
            self.ues.loc[ue_id, 'allocatedCell'] = cell_id
            # reward = 1  # Managed UE successfully
        # else:
        #     reward = -1  # Overloaded base station
        #
        reward, detailed_rewards = self.get_reward(ue, cell)

        self.current_step += 1

        info = {"reward_info": detailed_rewards}

        done = self.current_step >= len(self.ues)

        return self.get_state(), reward, done, info

    def get_reward(self, ue, allocated_cell):
        r1 = 0
        r2 = 0
        r3 = 0
        # r4 = 0

        if allocated_cell is not None:
            ue_slice = str(ue['slice'])
            cell = cells.iloc[allocated_cell['id']-1]
            cellType = ""
            if cell['anchored'] == 1:
                cellType = "anchored"
            elif cell['pico'] == 1:
                cellType = "pico"
            else:
                cellType = "macro"

            preference = policy[ue_slice]
            if cellType == preference['p1']:
                r1 += 100
            elif cellType == preference['p2']:
                r1 += 50
            elif cellType == preference['p3']:
                r1 += 25
            if allocated_cell['currCapacity'] < 0:
                r1 -= 100
        
        else:
            r2 -= 50  # Adjust the penalty for unmanaged users here

        # Include the unmanaged users in the reward
        # r4 = -0.1 * (self.get_unmanaged())  # Adjust the penalty for unmanaged users here


        total_capacity = sum(self.cells['maxCapacity-Kbps'])
        current_load = sum(self.cells['maxCapacity-Kbps'] - self.cells['currCapacity'])
        load_balance = current_load / total_capacity

        if load_balance > 0.95:
            r3 -= 20
        elif load_balance < 0.05:
            r3 -= 10
        else:
            r3 += 15

        ueWeight=1
        baseStationWeight=1
        reward = ueWeight*(r1+ r2) + baseStationWeight*r3

        return reward, [r1, r2, r3]


    def get_state(self):
        ue_allocations = self.ues['allocatedCell'].values
        cell_capacities = self.cells['currCapacity'].values
        state = np.concatenate((ue_allocations, cell_capacities))
        return state

    def get_unmanaged(self):
        return (self.ues['allocatedCell'] == 0).sum()

import torch
import torch.nn as nn
import torch.optim as optim
import torch.autograd as autograd
import torch.nn.functional as F
from collections import deque

USE_CUDA = torch.cuda.is_available()
dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor

class Variable(autograd.Variable):
    def __init__(self, data, *args, **kwargs):
        if USE_CUDA:
            data = data.cuda()
        super(Variable, self).__init__(data, *args, **kwargs)


class DQN(nn.Module):
    def __init__(self, state_size, action_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_size, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2500*10)
        self.gamma = 0.0  # discount rate
        self.epsilon = 1.0  # exploration rate
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.9995
        self.learning_rate = 0.001
        self.model = DQN(state_size, action_size)
        self.target_model = DQN(state_size, action_size)
        self.update_target_model()
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)

    def update_target_model(self):
        self.target_model.load_state_dict(self.model.state_dict())

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        state = torch.FloatTensor(state).unsqueeze(0)
        act_values = self.model(state)
        return torch.argmax(act_values[0]).item()

    def _sample_n_unique(self, sampling_f, n):
        """Helper function. Given a function `sampling_f` that returns
        comparable objects, sample n such unique objects.
        """
        res = []
        while len(res) < n:
            candidate = sampling_f()
            if candidate not in res:
                res.append(candidate)
        return res

    def _encode_sample(self, idxes):
        obs_batch      = np.array([self.memory[idx][0] for idx in idxes], dtype=np.float32)
        act_batch      = np.array([self.memory[idx][1] for idx in idxes], dtype=np.float32)
        rew_batch      = np.array([self.memory[idx][2] for idx in idxes], dtype=np.float32)
        next_obs_batch = np.array([self.memory[idx][3] for idx in idxes], dtype=np.float32)
        done_mask      = np.array([1.0 if self.memory[idx][4] else 0.0 for idx in idxes], dtype=np.float32)

        return obs_batch, act_batch, rew_batch, next_obs_batch, done_mask


    def replay(self, batch_size):

        # minibatch = random.sample(self.memory, batch_size)
        # for state, action, reward, next_state, done in minibatch:
        #     target = reward
        #     if not done:
        #         next_state = torch.FloatTensor(next_state).unsqueeze(0)
        #         target = reward + self.gamma * torch.max(self.target_model(next_state)[0])
        #     state = torch.FloatTensor(state).unsqueeze(0)
        #     target_f = self.model(state)
        #     target_f[0][action] = target
        #     self.optimizer.zero_grad()
        #     loss = F.mse_loss(target_f, self.model(state))
        #     loss.backward()
        #     self.optimizer.step()

        idxes = self._sample_n_unique(lambda: random.randint(0, len(self.memory) - 2), batch_size)
        obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = self._encode_sample(idxes)
        obs_batch = Variable(torch.from_numpy(obs_batch).type(dtype))
        act_batch = Variable(torch.from_numpy(act_batch).long())
        rew_batch = Variable(torch.from_numpy(rew_batch))
        next_obs_batch = Variable(torch.from_numpy(next_obs_batch).type(dtype))
        not_done_mask = Variable(torch.from_numpy(1 - done_mask)).type(dtype)
        current_Q_values = self.model(obs_batch).gather(1, act_batch.unsqueeze(1))
        # Compute next Q value based on which action gives max Q values
        # Detach variable from the current graph since we don't want gradients for next Q to propagated
        next_max_q = self.target_model(next_obs_batch).detach().max(1)[0]
        next_Q_values = not_done_mask * next_max_q
        # Compute the target of the current Q values
        target_Q_values = rew_batch + (self.gamma * next_Q_values)
        # Compute Bellman error
        bellman_error = target_Q_values - current_Q_values.squeeze(1)
        # clip the bellman error between [-1 , 1]
        # clipped_bellman_error = bellman_error.clamp(-1, 1)
        # Note: clipped_bellman_delta * -1 will be right gradient
        # d_error = bellman_error * -1.0
        # Clear previous gradients before backward pass
        loss = F.mse_loss(target_Q_values, current_Q_values.squeeze(1))
        self.optimizer.zero_grad()
        # run backward pass
        loss.backward()
        # current_Q_values.backward(d_error.data.unsqueeze(1))

        self.optimizer.step()

        return bellman_error.data.detach().mean().item()

    def load(self, name):
        self.model.load_state_dict(torch.load(name))

    def save(self, name):
        torch.save(self.model.state_dict(), name)

import pandas as pd

def train_dqn(ues, cells, episodes=1000, batch_size=32):
    env = Environment(ues, cells)
    state_size = len(env.get_state())
    action_size = len(ues) * len(cells)
    agent = DQNAgent(state_size, action_size)
    episodicRewards = []
    for e in range(episodes):
        rewards = []
        env.reset()
        state = env.get_state()
        for time in range(len(ues)):
            action = agent.act(state)
            ue_id = action // len(cells)
            cell_id = action % len(cells)
            next_state, reward, done, info = env.step((ue_id, cell_id))
            agent.remember(state, action, reward, next_state, done)
            state = next_state
            rewards.append(reward)
            if done:
                print(f"Episode {e}/{episodes}, unmanaged UEs: {env.get_unmanaged()}, rewards: {np.array(rewards).mean()}")
                # break
            if (time % 50 == 0):
                agent.update_target_model()

            if (len(agent.memory) > action_size * 5):  # update end the end of an episode, less frequently to avoid overfitting
                if (time % 5 == 0):
                    loss = agent.replay(batch_size)
                if agent.epsilon > agent.epsilon_min and (time % 10 == 0):
                    agent.epsilon *= agent.epsilon_decay
                if (time % 100 == 0):
                    print("\t\ttraining {}/{}, loss: {}, epsilon: {}, info: {}".format(time, len(ues), loss, agent.epsilon, info['reward_info']))
        episodicRewards.append(np.sum(rewards))
    return agent, episodicRewards

# Example usage
# ues and cells should be pandas DataFrames as defined in the problem
ues = pd.read_csv('data/data.csv')
cells = pd.read_csv('data/cells.csv')
# cells = cells.rename(columns={"maxCapacity-Kbps": "totalCapacity"})
agent, eR = train_dqn(ues, cells)
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 5))
plt.plot(eR, label='Total Reward per Episode')
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Training Progress')
plt.legend()
plt.show()
